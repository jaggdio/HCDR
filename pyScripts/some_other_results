# https://github.com/Microsoft/LightGBM/tree/master/examples
# https://github.com/Microsoft/LightGBM

https://github.com/flowlight0/talkingdata-adtracking-fraud-detection


# Links for tuning

# https://www.kaggle.com/garethjns/microsoft-lightgbm-with-parameter-tuning-0-823
# https://www.kaggle.com/chocozzz/lightgbm-parameter-tuning/code

## https://www.kaggle.com/nanomathias/bayesian-optimization-of-xgboost-lb-0-9769
## https://github.com/Microsoft/LightGBM/issues/1339


# unbalanced data
## https://github.com/Microsoft/LightGBM/issues/695

## xg boost tuning
https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
https://www.kaggle.com/naokishibuya/xgboot-with-gridsearchcv


#https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc

#
http://blairhudson.com/blog/posts/optimising-hyper-parameters-efficiently-with-scikit-optimize/

# train_test_split_example
https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html

# extra
https://pythonexample.com/code/xgboost-binary-classification/


# unbalanced data
https://www.kaggle.com/pranav84/lightgbm-fixing-unbalanced-data-lb-0-9680

# check LB
https://www.kaggle.com/abhinav97dutt/lightgbm-parameter-bayesian-optimization


valid_0's auc: 0.786112

Unknown parameter: gamma
[LightGBM] [Warning] Unknown parameter: colsample_bylevel
[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves


# com res
https://medium.com/data-design/bosch-competition-a-data-science-project-mode-8th-place-solution-team-lajmburo-df8c345f9181

validation = best_est.predict_proba(X_train)
print("Roc AUC: ", roc_auc_score(y_train, validation[:,1], average='macro'))

# CLustering
https://www.learndatasci.com/tutorials/k-means-clustering-algorithms-python-intro/

 X_train.to_csv('X_train.csv')

